{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from skimage import io, color, img_as_ubyte\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import models\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "model = VGGFace(model='resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class glcm:\n",
    "    def __init__(self, image):\n",
    "        distance = [1, 2, 3]\n",
    "        angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "        self.image = img_as_ubyte(image.astype('int64'))\n",
    "        self.glcm_mat = greycomatrix(self.image, distances = distance, angles = angles, symmetric = True, normed = True)\n",
    "        self.properties = ['correlation', 'homogeneity', 'contrast', 'energy']\n",
    "            \n",
    "    def correlation(self):\n",
    "        return greycoprops(self.glcm_mat, 'correlation').flatten()\n",
    "    \n",
    "    def homogeneity(self):\n",
    "        return greycoprops(self.glcm_mat, 'homogeneity').flatten()\n",
    "    \n",
    "    def contrast(self):\n",
    "        return greycoprops(self.glcm_mat, 'contrast').flatten()\n",
    "    \n",
    "    def energy(self):\n",
    "        return greycoprops(self.glcm_mat, 'energy').flatten()\n",
    "    \n",
    "    def glcm_all(self):\n",
    "        return np.hstack([greycoprops(self.glcm_mat, props).ravel() for props in self.properties])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_glcm_feature(img):\n",
    "    feats = glcm(img)\n",
    "    # energy\n",
    "    energy = feats.energy()\n",
    "    # correlation\n",
    "    corr = feats.correlation()\n",
    "    # contrast\n",
    "    cont = feats.contrast()\n",
    "    # homogeneity\n",
    "    homogeneity = feats.homogeneity()\n",
    "    # all the features at once\n",
    "    _all = feats.glcm_all()\n",
    "    return {\"energy\":energy,\n",
    "            \"corr\":corr, \n",
    "            \"cont\":cont, \n",
    "            \"homogeneity\":homogeneity, \n",
    "            \"_all\":_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96079968  0.9787254   0.97024738  0.9134136   0.90049676  0.9787254\n",
      "  0.92006303  0.9134136   0.85896475  0.94695266  0.87519117  0.83035806\n",
      "  0.80638675  0.79633071  0.81262921  0.77740831  0.76290263  0.79633071\n",
      "  0.76389014  0.77740831  0.75714175  0.77333179  0.75199753  0.7528404\n",
      "  4.50783442  2.44463226  3.41902997  9.94952432 11.44172074  2.44463226\n",
      "  9.17909516  9.94952432 16.21652466  6.09067307 14.32102435 19.47765336\n",
      "  0.14650718  0.14226142  0.14792634  0.13946595  0.13601599  0.14226142\n",
      "  0.13595822  0.13946595  0.13545363  0.13789257  0.13394757  0.13449008]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "\n",
    "Qry = Image.open('bg-4.jpg')\n",
    "Qry = Qry.convert(\"RGB\")\n",
    "Qry = cv2.cvtColor(np.asarray(Qry), cv2.COLOR_RGB2BGR)\n",
    "Qry = cv2.cvtColor(Qry, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "print(extract_glcm_feature(Qry)['_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_lista', 'x_listananthan', 'x_listb']\n",
      "['y_lista', 'y_listananthan', 'y_listb']\n"
     ]
    }
   ],
   "source": [
    "x_classList = []\n",
    "y_classList = []\n",
    "base_path='images/'\n",
    "source_path=base_path\n",
    "for child in os.listdir(source_path):\n",
    "#     print(child)\n",
    "    X_list = 'x_list'+str(child)\n",
    "    Y_list = 'y_list'+str(child)\n",
    "    vars()[X_list] = []\n",
    "    vars()[Y_list] = []\n",
    "    x_classList.append(X_list)\n",
    "    y_classList.append(Y_list)\n",
    "    sub_path = os.path.join(source_path, child)\n",
    "    bsub_path = os.path.join(base_path, child)\n",
    "    if os.path.isdir(sub_path):\n",
    "        for data_file in os.listdir(sub_path):\n",
    "            Qry = Image.open(os.path.join(sub_path, data_file))\n",
    "            Qry = Qry.convert(\"RGB\")\n",
    "            vars()[X_list].append(Qry)\n",
    "            vars()[Y_list].append(child)\n",
    "#     print(vars()[X_list])\n",
    "#     print(vars()[Y_list])\n",
    "print(x_classList)\n",
    "print(y_classList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int64 to uint8 without scaling because max value 167 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int64 to uint8 without scaling because max value 165 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int64 to uint8 without scaling because max value 181 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int64 to uint8 without scaling because max value 183 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int64 to uint8 without scaling because max value 185 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int64 to uint8 without scaling because max value 128 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int64 to uint8 without scaling because max value 209 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int64 to uint8 without scaling because max value 159 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.39999999999999997\n",
      "Mean Squared Error: 0.225\n",
      "Root Mean Squared Error: 0.4743416490252569\n",
      "[array([8.0949603e-06, 3.9089591e-07, 9.5817610e-05, ..., 2.6781387e-07,\n",
      "       1.3877601e-06, 1.1194167e-05], dtype=float32), array([2.6920701e-05, 8.5489864e-07, 2.4958616e-04, ..., 4.2944387e-07,\n",
      "       1.8932816e-06, 1.1624454e-05], dtype=float32)] [[0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 100)               863200    \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 870,463\n",
      "Trainable params: 870,463\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 8631, 8631, 8631, 8631, 8631, 8631, 8631\n  y sizes: 7\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-1e9b4433e42a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mtbCallBack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.\\Graph'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhistogram_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwrite_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mdnnModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtbCallBack\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mtestloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestAccuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdnnModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m           model=self)\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[0;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 8631, 8631, 8631, 8631, 8631, 8631, 8631\n  y sizes: 7\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "cnnCount = 0\n",
    "ranCount = 0\n",
    "for i in range(5):\n",
    "    X = []\n",
    "    for clas in x_classList:\n",
    "        sample = random.sample(vars()[clas], 3)\n",
    "        X.extend(sample)\n",
    "#     print(X)\n",
    "\n",
    "    Y = []\n",
    "    for clas in y_classList:\n",
    "        sample = random.sample(vars()[clas], 3)\n",
    "        Y.extend(sample)\n",
    "#     print(Y)\n",
    "    labelBinarizer = LabelBinarizer()\n",
    "    y = labelBinarizer.fit_transform(Y)\n",
    "    \n",
    "    X_glcm = []\n",
    "    for img in X:\n",
    "        img = cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        X_glcm.append(extract_glcm_feature(img)['_all'])\n",
    "#     print(X_glcm)\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_glcm, y, test_size=0.2, random_state=0)\n",
    "    regressor = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X_cnn = []\n",
    "    for img in X:\n",
    "        img = np.array(img.resize((224,224)))\n",
    "        img = img.reshape([-1,224,224,3])\n",
    "        features_train=model.predict([img])\n",
    "        X_cnn.append(features_train.flatten())\n",
    "#     print(X_cnn)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_cnn, y, test_size=0.2, random_state=0)\n",
    "    print(X_test,y_test)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    #tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "    dnnModel=models.Sequential()\n",
    "    dnnModel.add(layers.Dense(100,activation=\"tanh\",input_shape=(8631,)))\n",
    "    dnnModel.add(layers.Dense(50,activation=\"relu\"))\n",
    "    dnnModel.add(layers.Dense(30,activation=\"tanh\"))\n",
    "    dnnModel.add(layers.Dense(20,activation=\"relu\"))\n",
    "    dnnModel.add(layers.Dense(3,activation=\"softmax\"))\n",
    "    dnnModel.summary()\n",
    "\n",
    "\n",
    "\n",
    "    dnnModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    tbCallBack=tf.keras.callbacks.TensorBoard(log_dir='.\\Graph',histogram_freq=0,write_graph=True,write_images=True)\n",
    "\n",
    "    dnnModel.fit(X_train,y_train,epochs=5,batch_size=4,callbacks=[tbCallBack])\n",
    "\n",
    "    testloss, testAccuracy=dnnModel.evaluate(X_test,y_test)\n",
    "\n",
    "    print(testAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
